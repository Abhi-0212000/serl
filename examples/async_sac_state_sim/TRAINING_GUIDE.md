# async_sac_state_sim.py â€” Complete Training Guide

This guide explains **exactly** what happens when you run this script, with clear visual diagrams for each stage.

---
## Overview: Two Separate Processes

```
Terminal 1                          Terminal 2
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  python ... py      â”‚            â”‚  python ... py      â”‚
â”‚    --learner        â”‚            â”‚    --actor          â”‚
â”‚    --ip localhost   â”‚            â”‚    --ip localhost   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

They run the **same Python script** but with different flags. They communicate over TCP network.

---
## Part 1: Startup Sequence (What Happens First)

### Step 1: You start LEARNER first
```bash
python async_sac_state_sim.py --learner --ip localhost
```

```
LEARNER STARTUP:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Create SAC Agent (random initial weights)          â”‚
â”‚    - Actor network (policy)                            â”‚
â”‚    - 2 Critic networks                                 â”‚
â”‚    - 2 Target critic networks                          â”‚
â”‚    - Temperature (Î±)                                   â”‚
â”‚                                                        â”‚
â”‚ 2. Create Replay Buffer (empty)                       â”‚
â”‚    capacity = 1,000,000 transitions                    â”‚
â”‚    (stores on GPU/CPU RAM)                             â”‚
â”‚                                                        â”‚
â”‚ 3. Start TrainerServer                                â”‚
â”‚    server.register_data_store("actor_env", buffer)    â”‚
â”‚    server.start(threaded=True)                         â”‚
â”‚    â†’ Listens on network for incoming data             â”‚
â”‚                                                        â”‚
â”‚ 4. WAIT (blocking)                                     â”‚
â”‚    while len(replay_buffer) < 300:  # training_starts â”‚
â”‚        sleep(1)                                        â”‚
â”‚                                                        â”‚
â”‚ Status: Server running, waiting for data...           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**What is replay buffer?**
- It's a **list of transitions** stored in RAM
- Each transition = `(observation, action, reward, next_observation, done, mask)`
- `capacity=1,000,000` means it can hold **1 million transitions** (not memory size!)
- Example: if observation is 20 floats, action is 7 floats, that's ~27*4 = 108 bytes per transition
- Total memory â‰ˆ 108 MB for 1M transitions (small!)

**Why wait for 300 samples?**
- Need some data before training can start
- Can't sample from empty buffer
- Actor will fill it in next step

---
### Step 2: You start ACTOR (in new terminal)
```bash
python async_sac_state_sim.py --actor --ip localhost
```

```
ACTOR STARTUP:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Create same SAC Agent (same random weights)         â”‚
â”‚                                                        â”‚
â”‚ 2. Create QueuedDataStore (local buffer)              â”‚
â”‚    size = 2000 transitions                             â”‚
â”‚                                                        â”‚
â”‚ 3. Create TrainerClient                               â”‚
â”‚    client.connect(ip, wait_for_server=True)            â”‚
â”‚    â†’ Connects to learner's server                     â”‚
â”‚                                                        â”‚
â”‚ 4. Register callback for policy updates               â”‚
â”‚    def update_params(params):                          â”‚
â”‚        agent.state.params = params  # replace weights  â”‚
â”‚    client.recv_network_callback(update_params)         â”‚
â”‚                                                        â”‚
â”‚ 5. Create environment                                  â”‚
â”‚    env = gym.make("HalfCheetah-v4")                    â”‚
â”‚                                                        â”‚
â”‚ 6. START collecting data (while learner waits)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---
## Part 2: Initial Data Collection (First 300 Steps)

### Understanding the Two Queues

**IMPORTANT: There are TWO separate storage locations:**

1. **Local Queue (Actor side)**: `QueuedDataStore(2000)`
   - Temporary buffer on actor process
   - Holds transitions until next flush
   - **Empties completely** every 30 steps when flushed
   - Max capacity: 2000 transitions (rarely fills up)

2. **Replay Buffer (Learner side)**: `replay_buffer`
   - Permanent storage for training
   - Receives data from actor flushes
   - **Never empties** (keeps accumulating)
   - Capacity: 1,000,000 transitions

```
ACTOR PROCESS (Steps 0-299):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ obs = env.reset()  # get initial observation           â”‚
â”‚                                                         â”‚
â”‚ for step in range(300):  # warmup phase                â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚     â”‚ Step A: Sample RANDOM action                   â”‚ â”‚
â”‚     â”‚   action = env.action_space.sample()           â”‚ â”‚
â”‚     â”‚   (no policy used yet - pure exploration)      â”‚ â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                         â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚     â”‚ Step B: Execute in environment                 â”‚ â”‚
â”‚     â”‚   next_obs, reward, done = env.step(action)    â”‚ â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                         â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚     â”‚ Step C: Store in LOCAL queue (actor side)     â”‚ â”‚
â”‚     â”‚   data_store.insert({                          â”‚ â”‚
â”‚     â”‚       observations: obs,        # 20D vector   â”‚ â”‚
â”‚     â”‚       actions: action,          # 7D vector    â”‚ â”‚
â”‚     â”‚       rewards: reward,          # 1 scalar     â”‚ â”‚
â”‚     â”‚       next_observations: next_obs,  # 20D      â”‚ â”‚
â”‚     â”‚       dones: done,              # bool         â”‚ â”‚
â”‚     â”‚       masks: 1.0 - done         # float        â”‚ â”‚
â”‚     â”‚   })                                           â”‚ â”‚
â”‚     â”‚   Local queue grows: 1, 2, 3, ... up to 30    â”‚ â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                         â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚     â”‚ Step D: Every 30 steps, FLUSH to learner      â”‚ â”‚
â”‚     â”‚   if step % 30 == 0:                           â”‚ â”‚
â”‚     â”‚       client.update()                          â”‚ â”‚
â”‚     â”‚         1. Takes ALL items from local queue    â”‚ â”‚
â”‚     â”‚         2. Sends them over TCP to learner      â”‚ â”‚
â”‚     â”‚         3. LOCAL QUEUE NOW EMPTY (0 items)     â”‚ â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚               â”‚                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â”‚ Network (TCP)
                â”‚ Sends ~30 transitions
                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LEARNER PROCESS (still waiting)                        â”‚
â”‚                                                        â”‚
â”‚ server receives data â†’ replay_buffer.insert()          â”‚
â”‚ (auto-wired via register_data_store)                   â”‚
â”‚                                                        â”‚
â”‚ After 1st flush (step 30):                             â”‚
â”‚   Replay buffer: [31 transitions]                      â”‚
â”‚   Progress: 31/300 (10%)                               â”‚
â”‚                                                        â”‚
â”‚ After 2nd flush (step 60):                             â”‚
â”‚   Replay buffer: [61 transitions]                      â”‚
â”‚   Progress: 61/300 (20%)                               â”‚
â”‚                                                        â”‚
â”‚ After 10th flush (step 300):                           â”‚
â”‚   Replay buffer: [301 transitions]                     â”‚
â”‚   Progress: 301/300 âœ“ READY TO TRAIN!                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Detailed Timeline of First 300 Steps:**
```
Step  Local Queue (actor)    Replay Buffer (learner)   Learner Status
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
0     [t0]                   []                        Waiting...
1     [t0,t1]                []                        Waiting...
29    [t0..t29]              []                        Waiting...
30    FLUSH! â†’ []            [t0..t30] (31 items)      Waiting 31/300
31    [t31]                  [t0..t30]                 Waiting...
59    [t31..t59]             [t0..t30]                 Waiting...
60    FLUSH! â†’ []            [t0..t60] (61 items)      Waiting 61/300
90    FLUSH! â†’ []            [t0..t90] (91 items)      Waiting 91/300
120   FLUSH! â†’ []            [t0..t120] (121 items)    Waiting 121/300
150   FLUSH! â†’ []            [t0..t150] (151 items)    Waiting 151/300
180   FLUSH! â†’ []            [t0..t180] (181 items)    Waiting 181/300
210   FLUSH! â†’ []            [t0..t210] (211 items)    Waiting 211/300
240   FLUSH! â†’ []            [t0..t240] (241 items)    Waiting 241/300
270   FLUSH! â†’ []            [t0..t270] (271 items)    Waiting 271/300
300   FLUSH! â†’ []            [t0..t300] (301 items)    STARTS TRAINING! âœ“
```

**Visual: Two-Queue System**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ACTOR PROCESS                            â”‚
â”‚                                                             â”‚
â”‚  Step 0-29: Collecting...                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚  â”‚ Local Queue (QueuedDataStore) â”‚ â† TEMPORARY              â”‚
â”‚  â”‚ [t0, t1, t2, ..., t29, t30]   â”‚    (empties on flush)    â”‚
â”‚  â”‚ Size: 31/2000                 â”‚                          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                                                             â”‚
â”‚  Step 30: client.update() â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  (Takes all 31 items and sends)         â”‚                   â”‚
â”‚                                         â”‚                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚                   â”‚
â”‚  â”‚ Local Queue AFTER flush       â”‚     â”‚                   â”‚
â”‚  â”‚ []  â† EMPTY!                  â”‚     â”‚                   â”‚
â”‚  â”‚ Size: 0/2000                  â”‚     â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
                                         â”‚ TCP (31 transitions)
                                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   LEARNER PROCESS                           â”‚
â”‚                                                             â”‚
â”‚  server.register_data_store("actor_env", replay_buffer)    â”‚
â”‚      â†“ (auto-inserts received data)                         â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚ Replay Buffer (permanent storage)  â”‚ â† ACCUMULATES       â”‚
â”‚  â”‚ [t0, t1, ..., t30]                 â”‚    (never empties)  â”‚
â”‚  â”‚ Size: 31/1,000,000                 â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                                                             â”‚
â”‚  while len(replay_buffer) < 300:                            â”‚
â”‚      time.sleep(1)  # Keep waiting...                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

... 9 more flushes happen (steps 60, 90, 120, ..., 300) ...

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   LEARNER PROCESS                           â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚ Replay Buffer (after 10 flushes)   â”‚                     â”‚
â”‚  â”‚ [t0, t1, ..., t299, t300]          â”‚                     â”‚
â”‚  â”‚ Size: 301/1,000,000                â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                                                             â”‚
â”‚  while len(replay_buffer) < 300:  â† FALSE! (301 >= 300)    â”‚
â”‚                                                             â”‚
â”‚  server.publish_network(agent.state.params)                 â”‚
â”‚  print("sent initial network to actor")                    â”‚
â”‚                                                             â”‚
â”‚  âœ“ START TRAINING LOOP!                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---
## Part 3: Learner Starts Training

Once replay buffer has 300 transitions:

```
LEARNER PROCESS:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Replay buffer filled! (300/300)                        â”‚
â”‚                                                        â”‚
â”‚ server.publish_network(agent.state.params)             â”‚
â”‚   â†’ Sends initial policy to actor                     â”‚
â”‚                                                        â”‚
â”‚ print("sent initial network to actor")                â”‚
â”‚                                                        â”‚
â”‚ for step in range(1,000,000):  # main training loop   â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚     â”‚ STEP 1: Sample batch from replay buffer      â”‚  â”‚
â”‚     â”‚                                              â”‚  â”‚
â”‚     â”‚ batch_size = 256                             â”‚  â”‚
â”‚     â”‚ utd_ratio = 8  (critic_actor_ratio flag)     â”‚  â”‚
â”‚     â”‚ total_batch = 256 * 8 = 2048 transitions     â”‚  â”‚
â”‚     â”‚                                              â”‚  â”‚
â”‚     â”‚ batch = replay_buffer.sample(2048)           â”‚  â”‚
â”‚     â”‚   â†’ randomly pick 2048 transitions           â”‚  â”‚
â”‚     â”‚   â†’ each is (obs, action, reward, next_obs)  â”‚  â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                        â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚     â”‚ STEP 2: Update agent (SAC training)          â”‚  â”‚
â”‚     â”‚                                              â”‚  â”‚
â”‚     â”‚ agent.update_high_utd(batch, utd_ratio=8)    â”‚  â”‚
â”‚     â”‚   (see detailed breakdown below)             â”‚  â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                        â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚     â”‚ STEP 3: Publish updated policy               â”‚  â”‚
â”‚     â”‚                                              â”‚  â”‚
â”‚     â”‚ server.publish_network(agent.state.params)   â”‚  â”‚
â”‚     â”‚   â†’ Actor receives new weights               â”‚  â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                        â”‚
â”‚ Logs: critic_loss, actor_loss, temperature â†’ WandB    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---
## Part 4: What is UTD Ratio? (The 8:1 Update Pattern)

**UTD = Update-To-Data ratio** = how many gradient updates per environment sample

```
batch_size = 256
utd_ratio = 8
total_samples = 256 * 8 = 2048 transitions sampled from replay buffer
```

**What `agent.update_high_utd(batch, utd_ratio=8)` does:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Input: batch of 2048 transitions                          â”‚
â”‚                                                            â”‚
â”‚ STEP 1: Split into 8 mini-batches of 256 each             â”‚
â”‚   mini_batch_0 = batch[0:256]                              â”‚
â”‚   mini_batch_1 = batch[256:512]                            â”‚
â”‚   mini_batch_2 = batch[512:768]                            â”‚
â”‚   ...                                                      â”‚
â”‚   mini_batch_7 = batch[1792:2048]                          â”‚
â”‚                                                            â”‚
â”‚ STEP 2: Update CRITICS 8 times (once per mini-batch)      â”‚
â”‚   for i in range(8):                                       â”‚
â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚       â”‚ mini_batch = batch[i]  (256 transitions)   â”‚      â”‚
â”‚       â”‚                                            â”‚      â”‚
â”‚       â”‚ # Compute target (what Q should predict)   â”‚      â”‚
â”‚       â”‚ next_actions = policy(next_obs)            â”‚      â”‚
â”‚       â”‚ Q_target_1 = target_critic_1(next_obs, aâ€²) â”‚      â”‚
â”‚       â”‚ Q_target_2 = target_critic_2(next_obs, aâ€²) â”‚      â”‚
â”‚       â”‚ Q_min = min(Q_target_1, Q_target_2)        â”‚      â”‚
â”‚       â”‚ y = reward + 0.99*(1-done)*Q_min           â”‚      â”‚
â”‚       â”‚     - Î±*log Ï€(aâ€²|sâ€²)  # entropy term       â”‚      â”‚
â”‚       â”‚                                            â”‚      â”‚
â”‚       â”‚ # Update both critics                      â”‚      â”‚
â”‚       â”‚ Q1_pred = critic_1(obs, action)            â”‚      â”‚
â”‚       â”‚ Q2_pred = critic_2(obs, action)            â”‚      â”‚
â”‚       â”‚ loss = MSE(Q1_pred, y) + MSE(Q2_pred, y)   â”‚      â”‚
â”‚       â”‚ critics â† critics - lr * âˆ‡loss             â”‚      â”‚
â”‚       â”‚                                            â”‚      â”‚
â”‚       â”‚ # Soft update target networks              â”‚      â”‚
â”‚       â”‚ target â† 0.995*target + 0.005*critic       â”‚      â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                            â”‚
â”‚ STEP 3: Update ACTOR once (on full 2048 batch)            â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   â”‚ actions, log_probs = policy(obs)               â”‚      â”‚
â”‚   â”‚ Q1 = critic_1(obs, actions)                    â”‚      â”‚
â”‚   â”‚ Q2 = critic_2(obs, actions)                    â”‚      â”‚
â”‚   â”‚ Q_mean = (Q1 + Q2) / 2                         â”‚      â”‚
â”‚   â”‚                                                â”‚      â”‚
â”‚   â”‚ loss = mean(Î±*log_probs - Q_mean)              â”‚      â”‚
â”‚   â”‚ policy â† policy - lr * âˆ‡loss                   â”‚      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                            â”‚
â”‚ STEP 4: Update TEMPERATURE once (on full 2048 batch)      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   â”‚ entropy = -mean(log_probs)                     â”‚      â”‚
â”‚   â”‚ target_entropy = -3.5  # typically -action_dim/2â”‚      â”‚
â”‚   â”‚                                                â”‚      â”‚
â”‚   â”‚ loss = Î± * (entropy - target_entropy)          â”‚      â”‚
â”‚   â”‚ Î± â† Î± - lr * âˆ‡loss                             â”‚      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                            â”‚
â”‚ Output: updated agent (new weights for all networks)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why 8 critic updates but only 1 actor update?**
- Critics learn faster and benefit from more updates
- Actor can overfit if updated too often on same data
- 8:1 ratio gives better sample efficiency (learn more from same data)

---
## Part 5: Actor Continues Collecting (With Updated Policy)

```
ACTOR PROCESS (After receiving initial policy):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ for step in range(300, 1,000,000):  # continue         â”‚
â”‚                                                         â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚     â”‚ Step A: Sample action from POLICY (not random) â”‚ â”‚
â”‚     â”‚   rng, key = jax.random.split(rng)             â”‚ â”‚
â”‚     â”‚   action = agent.sample_actions(               â”‚ â”‚
â”‚     â”‚       obs, seed=key, deterministic=False       â”‚ â”‚
â”‚     â”‚   )                                            â”‚ â”‚
â”‚     â”‚   â†’ Uses current policy weights                â”‚ â”‚
â”‚     â”‚   â†’ Stochastic (explores via Gaussian noise)  â”‚ â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                         â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚     â”‚ Step B: Execute in environment                 â”‚ â”‚
â”‚     â”‚   next_obs, reward, done = env.step(action)    â”‚ â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                         â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚     â”‚ Step C: Store in local queue                   â”‚ â”‚
â”‚     â”‚   data_store.insert(transition)                â”‚ â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                         â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚     â”‚ Step D: Every 30 steps                         â”‚ â”‚
â”‚     â”‚   client.update()                              â”‚ â”‚
â”‚     â”‚     1. Flush buffered data â†’ learner           â”‚ â”‚
â”‚     â”‚     2. Receive new policy params â† learner     â”‚ â”‚
â”‚     â”‚     3. update_params() callback triggered      â”‚ â”‚
â”‚     â”‚        agent.state.params = new_params         â”‚ â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                         â”‚
â”‚ Result: Always using latest policy (or close to it)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Policy update frequency:**
- Learner publishes new policy **after every training step** (every 0.1-1 seconds)
- Actor receives updates **every 30 environment steps** (every ~0.6 seconds)
- Actor might lag behind by 30 steps max, but that's OK (off-policy learning)

---
## Part 6: Complete Data Flow (Ongoing Training)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     COMPLETE CYCLE                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Actor (Step N):
  obs â†’ policy â†’ action â†’ env â†’ (obs, act, rew, next_obs, done)
                                         â†“
                                 data_store.insert()
                                         â†“
                            [Queue: 30 transitions]
                                         â†“
                         (every 30 steps: client.update())
                                         â†“
                                    â•â•â•TCPâ•â•â•
                                         â†“
Learner:                                 â†“
                        server receives 30 transitions
                                         â†“
                         replay_buffer.insert(30)
                                         â†“
                    [Buffer: 300 â†’ 301 â†’ ... â†’ 1,000,000]
                                         â”‚
                                         â”‚ (continuously sample)
                                         â†“
                         batch = buffer.sample(2048)
                                         â†“
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚ agent.update_high_utd  â”‚
                         â”‚   8Ã— critic updates    â”‚
                         â”‚   1Ã— actor update      â”‚
                         â”‚   1Ã— temp update       â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚ new policy params
                                  â†“
                    server.publish_network(params)
                                  â†“
                             â•â•â•TCPâ•â•â•
                                  â†“
Actor:                            â†“
              client receives new params
                                  â†“
              update_params(params) callback
                                  â†“
              agent.state.params â† new_params
                                  â†“
              Continue collecting with better policy
                                  â†“
                            [cycle repeats]
```

---
## Part 7: Batch Size Explained Simply

**What is a batch?**
A batch is a **collection of transitions** sampled from the replay buffer.

**Example transition:**
```python
{
    'observations': [1.2, 0.5, -0.3, ...],      # 20 floats (robot state)
    'actions': [0.1, -0.2, 0.5, ...],           # 7 floats (motor commands)
    'rewards': 2.3,                             # 1 float (reward from env)
    'next_observations': [1.3, 0.6, -0.2, ...], # 20 floats (next state)
    'dones': False,                             # 1 bool (episode ended?)
    'masks': 1.0                                # 1 float (1.0 - done)
}
```

**batch_size = 256 means:**
```python
batch = {
    'observations': array of shape (256, 20),      # 256 robot states
    'actions': array of shape (256, 7),            # 256 actions taken
    'rewards': array of shape (256,),              # 256 rewards
    'next_observations': array of shape (256, 20), # 256 next states
    'dones': array of shape (256,),                # 256 booleans
    'masks': array of shape (256,)                 # 256 floats
}
```

**With utd_ratio=8:**
```python
total_batch = batch_size * utd_ratio = 256 * 8 = 2048 transitions

# Split into 8 mini-batches:
mini_batch_0 = batch[0:256]      # first 256 transitions
mini_batch_1 = batch[256:512]    # next 256 transitions
...
mini_batch_7 = batch[1792:2048]  # last 256 transitions
```

Each mini-batch is used for one critic update. Then the full batch (2048) is used for one actor update.

---
## Part 8: Replay Buffer Capacity

```
replay_buffer_capacity = 1,000,000 transitions
```

**What happens as it fills up?**
```
Step 0-300:       [###                    ] 300/1,000,000 (0.03%)
Step 1000:        [##########             ] 1,000/1,000,000 (0.1%)
Step 10,000:      [####################   ] 10,000/1,000,000 (1%)
Step 100,000:     [###################### ] 100,000/1,000,000 (10%)
Step 1,000,000:   [########################] 1,000,000/1,000,000 (100% FULL)

After full:       Buffer becomes CIRCULAR (oldest data gets overwritten)
Step 1,000,001:   Buffer still 1,000,000 (removed transition 1, added 1,000,001)
```

**Why so large?**
- More diverse data = better learning
- Can sample from old experiences (off-policy)
- 1M transitions â‰ˆ 100-200 MB RAM (small for modern GPUs)

---
## Part 9: Real Hardware vs Simulation

### Simulation (HalfCheetah-v4)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Actor speed: ~100-200 Hz (fast)         â”‚
â”‚ Learner speed: ~500-1000 Hz (GPU)       â”‚
â”‚                                         â”‚
â”‚ No real-time constraints                â”‚
â”‚ Can run multiple actors in parallel     â”‚
â”‚ Training completes in hours             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Real Hardware (Franka Robot)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Actor speed: ~50 Hz (robot control rate)â”‚
â”‚ Learner speed: ~500-1000 Hz (same GPU)  â”‚
â”‚                                         â”‚
â”‚ REAL-TIME constraints (safety critical) â”‚
â”‚ Usually 1 actor (one physical robot)    â”‚
â”‚ Training takes days/weeks               â”‚
â”‚                                         â”‚
â”‚ Key difference: Actor is SLOW           â”‚
â”‚ â†’ High UTD ratio (8-20) is essential    â”‚
â”‚ â†’ Squeeze max learning from each sample â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why async is critical for real robots:**
```
Without async (blocking):
  collect sample (20ms) â†’ send to learner â†’ wait for training (100ms) 
  â†’ receive policy â†’ repeat
  Total: 120ms per sample â†’ 8 samples/sec â†’ VERY SLOW

With async (non-blocking):
  Actor: collect sample (20ms) â†’ queue â†’ collect next (20ms) â†’ ...
                                    â†“ (flush every 30 steps)
  Learner: â† receive batch â†’ train â†’ publish policy (actor doesn't wait)
  
  Result: 50 samples/sec from actor, 500 updates/sec from learner
```

---
## Part 10: Inference (Using Trained Policy)

After training, you can use the policy without the learner:

```python
# Load trained checkpoint
agent = SACAgent.load(checkpoint_path)

# Run policy in environment
obs = env.reset()
for step in range(1000):
    # Deterministic action (no exploration)
    action = agent.sample_actions(obs, argmax=True)
    
    obs, reward, done, info = env.step(action)
    
    if done:
        obs = env.reset()
```

**Key difference:**
- Training: `deterministic=False` (explores via Gaussian noise)
- Inference: `argmax=True` (uses mean action, no noise)

**What you DON'T need for inference:**
- Replay buffer
- Learner process
- Critic networks (only actor/policy is used)
- Temperature

**Minimal inference setup:**
```python
agent = SACAgent.load(checkpoint)
policy_fn = lambda obs: agent.sample_actions(obs, argmax=True)
# That's it! Just call policy_fn(obs) to get actions
```

---
## Part 11: Summary Timeline

```
Time     Learner Terminal              Actor Terminal
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
0:00     Start learner                 (not started)
         Create agent, buffer, server
         Wait for data...
         
0:05     (still waiting)               Start actor
                                       Connect to learner
                                       Collect random actions
                                       (steps 0-299)
                                       
0:10     Received 300 transitions!     (collecting step 300)
         Start training
         Publish initial policy   â”€â”€â”€â†’ Receive policy
         
0:11     Sample batch (2048)            Use policy for actions
         Update critics 8Ã—              (steps 301-330)
         Update actor 1Ã—
         Update temp 1Ã—
         Publish new policy      â”€â”€â”€â†’  Receive new policy
         
0:12     Sample batch (2048)            Continue collecting
         Train...                       (steps 331-360)
         Publish policy          â”€â”€â”€â†’  Receive policy
         
...      (training continues)           (collecting continues)
         ~1000 updates/sec              ~50-100 steps/sec
         
Hours later: Training complete         Stop actor
         Save checkpoint                Load checkpoint
                                       Run inference only
```

---
## Part 12: Key Takeaways

1. **Two processes**: Learner (trains) and Actor (collects data)
2. **Replay buffer**: Stores transitions (not memory size, but count of transitions)
3. **Batch**: 256 transitions sampled randomly from buffer
4. **UTD ratio**: 8 critic updates per 1 actor update (8:1 ratio)
5. **Total batch size**: `256 * 8 = 2048` transitions sampled per training step
6. **Buffer filling**: Actor sends data every 30 steps; learner waits until 300 transitions
7. **Policy sync**: Learner publishes after every update; actor receives every 30 steps
8. **Async benefit**: Actor never waits for learner; both run at max speed
9. **For real robots**: High UTD (8-20) essential because data collection is slow
10. **Inference**: Just load policy, no learner/buffer/critics needed

---
## Troubleshooting Visual

```
Problem: Learner stuck at "Filling up replay buffer 0/300"
Solution: Start actor! Learner can't train without data.

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Learner      â”‚       â”‚ Actor        â”‚
â”‚ Waiting...   â”‚       â”‚ NOT RUNNING  â”‚ â† START THIS!
â”‚ 0/300        â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Problem: Actor can't connect to learner
Solution: Start learner first! Actor needs server to be running.

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Learner      â”‚       â”‚ Actor        â”‚
â”‚ NOT RUNNING  â”‚ â† START THIS FIRST! â”‚ Connection refused â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Problem: Training unstable / diverging
Solution: Lower UTD ratio or increase batch size

critic_actor_ratio = 8  â†’ try 4
batch_size = 256        â†’ try 512
```

---
That's it! You now understand exactly how `async_sac_state_sim.py` works. ğŸ¯
